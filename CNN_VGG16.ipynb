{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406ed919-5dc4-4be0-9c81-92a8cea9e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bac4f03-9d2d-4a92-8b55-749b0df80659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and load a grayscale image \n",
    "# Attention: When I want smaller images like 64x64 I need to consider this in VGG16 - default is 224x224x3 with 3 channels for RGB\n",
    "\n",
    "def preprocess_grayscale_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load image as grayscale\n",
    "    image = cv2.resize(image, (64, 64))  # Resize image to 64x64\n",
    "    image = np.stack((image,)*3, axis=-1)  # Convert to 3-channel RGB\n",
    "    image = image.astype('float32')\n",
    "    return image\n",
    "\n",
    "# Function to extract features \n",
    "def extract_features(image_path):\n",
    "    image = preprocess_grayscale_image(image_path)\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    image = tf.keras.applications.vgg16.preprocess_input(image)  # Preprocess image\n",
    "    features = model.predict(image, verbose=0)\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db1fbf-9e26-48de-a92a-e666385c06f3",
   "metadata": {},
   "source": [
    "To use VGG16 with 64x64 grayscale images, you can follow these steps:\n",
    "1. Convert Grayscale to RGB: Duplicate the grayscale channel to create a 3-channel RGB image.\n",
    "2. Resize to 64x64\n",
    "3. Modify the VGG16 Architecture to handle 64x64 input size because the default is different (224x224x3 with 3 channels for RGB). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a798bc9-30b6-410b-948e-9a122fffb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels of the glaciers for the measurements\n",
    "file_20_grid = r'n0_metadata19_hmineq0.0_tmin20050000_mean_grid_20.csv'\n",
    "data_20_grid = pd.read_csv(file_20_grid, low_memory=False)\n",
    "RGIId_names = np.unique(data_20_grid['RGIId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9410c2e-1f9e-45bc-90db-5e34b88a9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(64, 64, 3))\n",
    "\n",
    "# Load the VGG16 model with weights, excluding the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_layer)\n",
    "\n",
    "# Create a model that outputs the features from the last convolutional block\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a51bef91-e1da-4e9a-97db-25e0d8f419f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████████████████████████████████████████| 2101/2101 [08:06<00:00,  4.32image/s]\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "path = r'glacier_geometries_images_small/'\n",
    "\n",
    "for name in tqdm(RGIId_names, desc=\"Processing images\", unit=\"image\"):\n",
    "    image_path = path + name + '.png'\n",
    "    features = extract_features(image_path)\n",
    "    features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "572bee58-bf08-49fd-a4d8-bd5103849fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 73111/73111 [00:22<00:00, 3234.24it/s]\n"
     ]
    }
   ],
   "source": [
    "picture_features = []\n",
    "for name in tqdm(data_20_grid['RGIId'], desc=\"Create Features for Measurements\", unit=\"measurement\"):\n",
    "    for i in range(len(RGIId_names)):\n",
    "        if name == RGIId_names[i]:\n",
    "            picture_features.append(features_list[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "211336c0-72de-4a27-b32a-1e4d8dbdea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "features_df = pd.DataFrame(picture_features)\n",
    "features_df.columns = [f'pic_flat_feat_{i+1}' for i in range(features_df.shape[1])]\n",
    "df = pd.concat([data_20_grid, features_df], axis=1)\n",
    "df.to_csv(\"n0_wpics_feat_metadata19_hmineq0.0_tmin20050000_mean_grid_20.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
